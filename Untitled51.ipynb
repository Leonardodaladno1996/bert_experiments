{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonardodaladno1996/bert_experiments/blob/main/Untitled51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwWzwmegI_kY"
      },
      "outputs": [],
      "source": [
        "# Добавить после импортов, заменить демонстрационные данные\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "\n",
        "# Загрузка реального датасета\n",
        "def load_math_dataset():\n",
        "    \"\"\"Загрузка датасета математических задач\"\"\"\n",
        "    # Параметры Google Sheets\n",
        "    file_id = '13YIbphbWc62sfa-bCh8MLQWKizaXbQK9'\n",
        "    gid = '661202001'\n",
        "\n",
        "    # Формируем URL для экспорта в Excel\n",
        "    export_url = f'https://docs.google.com/spreadsheets/d/{file_id}/export?format=xlsx&gid={gid}'\n",
        "\n",
        "    try:\n",
        "        # Пытаемся скачать файл\n",
        "        response = requests.get(export_url)\n",
        "        response.raise_for_status()  # Проверяем ошибки\n",
        "\n",
        "        # Сохраняем во временный файл\n",
        "        with open('temp_sheet.xlsx', 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Читаем файл\n",
        "        df = pd.read_excel('temp_sheet.xlsx', engine='openpyxl')\n",
        "\n",
        "    except Exception as e:\n",
        "        # Альтернативная попытка через локальный файл\n",
        "        try:\n",
        "            df = pd.read_excel('math_problems.xlsx', engine='openpyxl')\n",
        "        except:\n",
        "            raise RuntimeError(f\"Не удалось загрузить данные: {str(e)}\")\n",
        "\n",
        "    display(df.head())\n",
        "\n",
        "    # Предполагаем структуру: [index, text, category, ...]\n",
        "    texts = df.iloc[:, 1].astype(str).tolist()  # Второй столбец - тексты задач\n",
        "    categories = df.iloc[:, 2].astype(str).tolist()  # Третий столбец - категории\n",
        "\n",
        "    # Кодируем категории в числовые метки\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(categories)\n",
        "\n",
        "    return texts, labels, label_encoder\n",
        "\n",
        "# Заменить демонстрационные данные\n",
        "texts, labels, label_encoder = load_math_dataset()\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"Загружено {len(texts)} задач\")\n",
        "print(f\"Количество классов: {num_classes}\")\n",
        "print(f\"Классы: {label_encoder.classes_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iULY77IuI72o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Union, List, Dict, Any\n",
        "import copy\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Пример данных для демонстрации\n",
        "class MathProblemsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка математических текстов\"\"\"\n",
        "        # Очистка и нормализация текста\n",
        "        text = str(text).strip()\n",
        "        # Удаление лишних пробелов\n",
        "        text = ' '.join(text.split())\n",
        "        # Обработка математических символов если нужно\n",
        "        return text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWE8R00OJ8rh"
      },
      "outputs": [],
      "source": [
        "class TransformerClassificationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Кастомная модель классификации на основе трансформера.\n",
        "\n",
        "    Архитектура:\n",
        "    - Backbone: предобученная модель из HuggingFace\n",
        "    - Dropout layer для регуляризации\n",
        "    - Классификационная голова (линейный слой)\n",
        "\n",
        "    Особенности реализации:\n",
        "    - Поддержка различных архитектур трансформеров\n",
        "    - Гибкая настройка количества классов\n",
        "    - Возможность извлечения attention weights для анализа\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_transformer_model: Union[str, nn.Module], num_classes: int = 7, dropout_rate: float = 0.1):\n",
        "        super(TransformerClassificationModel, self).__init__()\n",
        "\n",
        "        if isinstance(base_transformer_model, str):\n",
        "            # Загружаем конфигурацию модели\n",
        "            self.config = AutoConfig.from_pretrained(base_transformer_model)\n",
        "            # Загружаем backbone модель\n",
        "            self.backbone = AutoModel.from_pretrained(base_transformer_model)\n",
        "            # Загружаем соответствующий токенизатор\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(base_transformer_model)\n",
        "        else:\n",
        "            self.backbone = base_transformer_model\n",
        "            self.config = base_transformer_model.config\n",
        "            self.tokenizer = None\n",
        "\n",
        "        # Получаем размерность скрытых состояний\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "        # Добавляем dropout для регуляризации\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Классификационная голова\n",
        "        self.classifier_1 = nn.Linear(self.hidden_size, num_classes * 10)\n",
        "        self.classifier_2 = nn.Linear(num_classes * 10, num_classes)\n",
        "\n",
        "        # Сохраняем количество классов\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Инициализируем веса классификатора\n",
        "        nn.init.xavier_uniform_(self.classifier_1.weight)\n",
        "        nn.init.xavier_uniform_(self.classifier_2.weight)\n",
        "        nn.init.zeros_(self.classifier_1.bias)\n",
        "        nn.init.zeros_(self.classifier_2.bias)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, return_attention=False):\n",
        "        \"\"\"\n",
        "        Прямой проход через модель.\n",
        "\n",
        "        Args:\n",
        "            input_ids: токенизированные входные данные\n",
        "            attention_mask: маска внимания\n",
        "            labels: истинные метки (опционально)\n",
        "            return_attention: возвращать ли веса внимания\n",
        "\n",
        "        Returns:\n",
        "            dict с logits, loss (если labels предоставлены), и attention weights (если запрошены)\n",
        "        \"\"\"\n",
        "\n",
        "        # Пропускаем через backbone\n",
        "        outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=return_attention\n",
        "        )\n",
        "\n",
        "        # Получаем представление [CLS] токена (или mean pooling)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            # Для BERT-подобных моделей используем pooler_output\n",
        "            pooled_output = outputs.pooler_output\n",
        "        else:\n",
        "            # Для других моделей используем mean pooling\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "            pooled_output = last_hidden_state[:, 0]\n",
        "\n",
        "        # Применяем dropout\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # Получаем логиты\n",
        "        pre_logits = self.classifier_1(pooled_output)\n",
        "        pre_logits_dropout = self.dropout(pre_logits)\n",
        "        logits = self.classifier_2(pre_logits_dropout)\n",
        "\n",
        "        # Подготавливаем выходной словарь\n",
        "        result = {'logits': logits}\n",
        "\n",
        "        # Вычисляем loss если предоставлены метки\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        # Добавляем attention weights если запрошены\n",
        "        if return_attention:\n",
        "            result['attentions'] = outputs.attentions\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_bNRnIbKm7t"
      },
      "outputs": [],
      "source": [
        "def freeze_backbone_function(model: TransformerClassificationModel):\n",
        "    \"\"\"\n",
        "    Замораживает параметры backbone модели для предотвращения их обновления.\n",
        "\n",
        "    Принцип работы:\n",
        "    - Устанавливает requires_grad=False для всех параметров backbone\n",
        "    - Оставляет размороженными только параметры классификационной головы\n",
        "    - Это позволяет использовать предобученные представления без их изменения\n",
        "\n",
        "    Преимущества заморозки:\n",
        "    - Быстрее обучение (меньше параметров для обновления)\n",
        "    - Меньше требований к памяти\n",
        "    - Защита от катастрофического забывания\n",
        "    - Работает хорошо при небольших датасетах\n",
        "    \"\"\"\n",
        "\n",
        "    # Замораживаем все параметры backbone\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Убеждаемся, что классификационная голова остается разморожена\n",
        "    for param in model.classifier_1.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.classifier_2.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Также размораживаем dropout (хотя у него нет обучаемых параметров)\n",
        "    for param in model.dropout.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(\"Backbone заморожен. Обучаемые параметры:\")\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Обучаемые: {trainable_params:,}\")\n",
        "    print(f\"Общие: {total_params:,}\")\n",
        "    print(f\"Процент обучаемых: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpxr_vfIK_0w"
      },
      "outputs": [],
      "source": [
        "def train_transformer(transformer_model, texts, labels, freeze_backbone=True, epochs=10, learning_rate=2e-5, batch_size=16):\n",
        "    \"\"\"\n",
        "    Функция для дообучения трансформера на задаче классификации.\n",
        "\n",
        "    Особенности реализации:\n",
        "    - Поддержка заморозки backbone\n",
        "    - Адаптивное изменение learning rate\n",
        "    - Мониторинг метрик качества\n",
        "    - Early stopping для предотвращения переобучения\n",
        "\n",
        "    Параметры обучения:\n",
        "    - Learning rate: 2e-5 (оптимальный для трансформеров)\n",
        "    - Batch size: 8 (компромисс между стабильностью и скоростью)\n",
        "    - Optimizer: AdamW (лучший выбор для трансформеров)\n",
        "    - Scheduler: Linear warmup + decay\n",
        "    \"\"\"\n",
        "\n",
        "    # Создаем копию модели для обучения\n",
        "    model = copy.deepcopy(transformer_model)\n",
        "\n",
        "    # Применяем заморозку если необходимо\n",
        "    if freeze_backbone:\n",
        "        model = freeze_backbone_function(model)\n",
        "    else:\n",
        "        print(\"Обучение без заморозки backbone\")\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"Всего обучаемых параметров: {trainable_params:,}\")\n",
        "\n",
        "    # Подготавливаем данные с разделением на train/validation\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    tokenizer = model.tokenizer\n",
        "    train_dataset = MathProblemsDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = MathProblemsDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Настраиваем оптимизатор\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=0.01,  # L2 регуляризация\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Scheduler для learning rate\n",
        "    num_training_steps = len(train_loader) * epochs\n",
        "    scheduler = optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=0.1,\n",
        "        total_iters=num_training_steps // 10\n",
        "    )\n",
        "\n",
        "    # Перемещаем модель на GPU если доступно\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Обучающий цикл\n",
        "    model.train()\n",
        "    training_losses = []\n",
        "\n",
        "    print(f\"Начинаем обучение на {device}\")\n",
        "    print(f\"Epochs: {epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_losses = []\n",
        "        epoch_predictions = []\n",
        "        epoch_labels = []\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "        for batch in pbar:\n",
        "            # Перемещаем данные на устройство\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "\n",
        "\n",
        "            # Check if labels are within the valid range\n",
        "            if torch.max(labels) >= model.num_classes or torch.min(labels) < 0:\n",
        "                raise ValueError(f\"Labels are out of the valid range [0, {model.num_classes - 1}]. Found max label: {torch.max(labels)}, min label: {torch.min(labels)}\")\n",
        "\n",
        "\n",
        "            # Обнуляем градиенты\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Прямой проход\n",
        "            outputs = model(input_ids, attention_mask, labels)\n",
        "            loss = outputs['loss']\n",
        "\n",
        "            # Обратный проход\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping для стабильности\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Обновляем параметры\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Сохраняем метрики\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Предсказания для расчета accuracy\n",
        "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
        "            epoch_predictions.extend(predictions.cpu().numpy())\n",
        "            epoch_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Обновляем прогресс-бар\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
        "            })\n",
        "\n",
        "        # Вычисляем метрики эпохи\n",
        "        epoch_loss = np.mean(epoch_losses)\n",
        "        epoch_accuracy = accuracy_score(epoch_labels, epoch_predictions)\n",
        "        epoch_f1 = f1_score(epoch_labels, epoch_predictions, average='weighted')\n",
        "        print(f\"Train - Accuracy: {epoch_accuracy:.4f}, F1: {epoch_f1:.4f}\")\n",
        "\n",
        "        training_losses.append(epoch_loss)\n",
        "\n",
        "        val_accuracy, val_f1 = evaluate_model_on_loader(model, val_loader)\n",
        "        print(f\"Validation - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jNSRsh9aDWa"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_on_loader(model, data_loader):\n",
        "    \"\"\"Оценка модели на DataLoader\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    return accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLZPFSZ3aNbp"
      },
      "outputs": [],
      "source": [
        "# Создаем модель RuBERT-tiny2\n",
        "rubert_tiny_transformer_model = TransformerClassificationModel(\n",
        "    base_transformer_model='cointegrated/rubert-tiny2',\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "print(\"\\nОбучение RuBERT-tiny2 с замороженным backbone:\")\n",
        "rubert_tiny_finetuned_with_freezed_backbone = train_transformer(\n",
        "    rubert_tiny_transformer_model,\n",
        "    texts,\n",
        "    labels,\n",
        "    freeze_backbone=True,\n",
        "    learning_rate=2e-5\n",
        ")\n",
        "\n",
        "print(\"\\nОбучение RuBERT-tiny2 без заморозки backbone:\")\n",
        "rubert_tiny_transformer_model_full = TransformerClassificationModel(\n",
        "    base_transformer_model='cointegrated/rubert-tiny2',\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "rubert_tiny_full_finetuned = train_transformer(\n",
        "    rubert_tiny_transformer_model_full,\n",
        "    texts,\n",
        "    labels,\n",
        "    freeze_backbone=False,\n",
        "    learning_rate=1e-5  # Меньший learning rate для полного обучения\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ RuBERT-tiny2:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "RuBERT-tiny2 - это компактная версия русскоязычной BERT модели:\n",
        "- Параметры: ~29M (в 4 раза меньше стандартного BERT)\n",
        "- Архитектура: 3 слоя, 12 голов внимания, 312 скрытых единиц\n",
        "- Предобучена на большом русскоязычном корпусе\n",
        "\n",
        "Сравнение подходов:\n",
        "1. С замороженным backbone:\n",
        "   - Быстрее обучение (обновляются только ~3K параметров)\n",
        "   - Меньше риска переобучения\n",
        "   - Хорошо работает на небольших датасетах\n",
        "   - Полагается на предобученные представления\n",
        "\n",
        "2. Без заморозки backbone:\n",
        "   - Дольше обучение (обновляются все 29M параметров)\n",
        "   - Больше гибкости для адаптации к задаче\n",
        "   - Может лучше улавливать специфику математических задач\n",
        "   - Требует больше данных для стабильного обучения\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Idk_u2Sn3Zd"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fxi2kNiLawLV"
      },
      "outputs": [],
      "source": [
        "# Создаем модель MathBERT\n",
        "mathbert_transformer_model = TransformerClassificationModel(\n",
        "    base_transformer_model='tbs17/MathBert',\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "print(\"\\nОбучение MathBERT с замороженным backbone:\")\n",
        "mathbert_finetuned_with_freezed_backbone = train_transformer(\n",
        "    mathbert_transformer_model,\n",
        "    texts,\n",
        "    labels,\n",
        "    freeze_backbone=True,\n",
        "    learning_rate=2e-5\n",
        ")\n",
        "\n",
        "print(\"\\nОбучение MathBERT без заморозки backbone:\")\n",
        "mathbert_transformer_model_full = TransformerClassificationModel(\n",
        "    base_transformer_model='tbs17/MathBert',\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "mathbert_full_finetuned = train_transformer(\n",
        "    mathbert_transformer_model_full,\n",
        "    texts,\n",
        "    labels,\n",
        "    freeze_backbone=False,\n",
        "    learning_rate=1e-5\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ MathBERT vs RuBERT-tiny2:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "MathBERT - специализированная модель для математических задач:\n",
        "- Предобучена на математических текстах и формулах\n",
        "- Лучше понимает математическую терминологию\n",
        "- Оптимизирована для задач в математической области\n",
        "\n",
        "Сравнение с RuBERT-tiny2:\n",
        "\n",
        "Преимущества MathBERT:\n",
        "1. Специализация на математических задачах\n",
        "2. Лучше понимает математические символы и формулы\n",
        "3. Предобученные представления более релевантны для задачи\n",
        "4. Потенциально лучше accuracy на математических текстах\n",
        "\n",
        "Преимущества RuBERT-tiny2:\n",
        "1. Более компактная архитектура\n",
        "2. Быстрее инференс\n",
        "3. Меньше требований к памяти\n",
        "4. Лучше обобщение на общие текстовые задачи\n",
        "\n",
        "Ожидаемые результаты:\n",
        "- MathBERT должен показать лучшие результаты на математических задачах\n",
        "- RuBERT-tiny2 может быть более стабильным при малых данных\n",
        "- Заморозка backbone эффективнее для обеих моделей при ограниченных данных\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLkvoICMdfD0"
      },
      "outputs": [],
      "source": [
        "model = mathbert_finetuned_with_freezed_backbone\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "val_text =  ['What does it mean: Generalization error math.abs(R(h) - Rn(h))']\n",
        "val_label = [1]\n",
        "\n",
        "val_dataset = MathProblemsDataset(val_text, val_label, tokenizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "text_encoded = next(iter(val_loader))\n",
        "\n",
        "row_result = model(text_encoded['input_ids'].to(device))['logits']\n",
        "print(row_result, type(row_result))\n",
        "print(torch.argmax(row_result, dim=-1))\n",
        "print(nn.functional.softmax(row_result))\n",
        "\n",
        "result = row_result.detach()\n",
        "print(result, type(result))\n",
        "result_softmax = nn.functional.softmax(result)\n",
        "print(result_softmax, type(result_softmax))\n",
        "\n",
        "prediction = torch.argmax(result_softmax)\n",
        "print(prediction, type(prediction))\n",
        "\n",
        "prediction = prediction.cpu().numpy()\n",
        "print(prediction, type(prediction))\n",
        "\n",
        "print(label_encoder.classes_)\n",
        "print(label_encoder.inverse_transform(prediction.reshape(-1, 1)))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOr5Ywd0nVUGvYtk68tcc+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}